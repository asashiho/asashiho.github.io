<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>azure on ASA Blog</title><link>https://asashiho.github.io/categories/azure/</link><description>Recent content in azure on ASA Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>Your Copyright</copyright><lastBuildDate>Thu, 30 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://asashiho.github.io/categories/azure/index.xml" rel="self" type="application/rss+xml"/><item><title>Spring WebFlux + KuberntesでアプリのGraceful Shutdownを実装する</title><link>https://asashiho.github.io/spring-graceful/spring-graceful/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/spring-graceful/spring-graceful/</guid><description>最近業務でお手伝いさせていただく案件のなかでも、Java on Kubernetesなパターンが増えてきています。Kubernetesに限らずクラウドのマネージドPaaSで動かすにはアプリケーション側でもクラウドネイティブを考慮した実装が必要になり、特に耐障害性や回復性などを意識した実装が大事になってきます。
クラウドネイティブなアプリケーション方式設計されたシステムでは、大きな問題もなく開発者からみると「クラウド超便利」「マネージド最高か！」となりますが、ごくまれに「オンプレで動いていたそのままのアプリをクラウドに持ってきた」or「オンプレを前提とした開発手法で実装されたレガシーアプリをクラウドネイティブなPaaSサービスで動かしてみた」というケースで問題が発生することがあります。
クラウドネイティブな環境でアプリケーションを動かすときに、開発者の観点でどういうところに注意すればよいかは
通信先のサービス(DB/外部API)は常に動いていると思うべからず 死に際を美しく(Graceful Shutdown) 秘匿情報・環境依存値の取り扱い ステートレスにすべし あたりが大事なポイントになってくるかなと思っています。
このエントリでは、2つ目の
死に際を美しく(Graceful Shutdown)
対策の一つであるアプリケーションのGraceful Shutdownの実装について、Spring Bootを使った実装の例を説明します。
そもそも、Graceful Shutdownとは？ 従来のオンプレでのWeb3層システムの場合、システムメンテナンスなどでWebサーバを停止させる場合、システム運用者によって次の作業を行います。
LBで該当Webサーバへの新規リクエストの受付を停止する 該当Webサーバ上で動くアプリケーションが処理し終わるのを待つ 処理中のプロセスが無いことを確認できたら、Webサーバを停止する クラウドネイティブな構成の場合、これらの処理をソフトウエアで自動化しています。たとえば、Kubernetesの場合、LBに相当するService/IngressからWebアプリケーションに相当するPodへのルーティングを宣言ベースで行い、クラスタの状態を維持します。またプラットフォームのメンテナンス等でPodが動いているWorker Nodeが移動するケースも普通にあります。また、Deploymentの更新などの理由でPodの再作成やPodの終了処理がいたるところで行われます。
これらは基盤メンテチーム側からみると「Kubernetesのしくみをつかって工数のかかるWebサーバのメンテナンスを自動化した！生産性向上！ゆっくり眠れる！！最高！！！！」となります。
一方のアプリ開発チーム側から見ると「Kubernetesが勝手にWebサーバ再起動した！！！こっちはサービス提供時間中だぞ！！！ひどい！！！今までは1台ずつ丁寧に夜間手作業でやってくれてたじゃないか！！！」
そしてここではじめて、システム運用者のありがたみを心から知ることになります。
つまり、、、、、このような悲惨なすれ違いをなくすには 「今までシステム運用者がやっていた1-3の作業はどうなる？」 を深堀りしたアプリケーション方式設計/基盤方式設計が必要だということがわかります。
Graceful Shutdownとは、アプリケーションをシャットダウンする前に、まだ進行中の要求を完了するためのタイムアウト期間を設け、その間に安全にアプリケーションを終了させることです。タイムアウト期間中に、すでに動いているアプリの処理の終了やDBなど外部サービスへの接続を停止する処理を行い、それが完了したらシャットダウンを行うことでデータのロストやコネクションのクローズ処理、アプリ不整合やエラーを防ぐことができます。
Spring boot 2.3.0.RELEASE以降では、フレームワーク自体にGraceful Shutdownの機能が追加されました。この機能は現在Tomcat/Undertow/Netty/JettyのWebサーバでされています。これは、SmartLifecycleBeanを停止する最初のフェーズで実行されます。
アプリケーションが新しい要求を停止する方法は、Webサーバによって異なりますが公式ドキュメントによると、Tomcat/Jetty/Nettyはネットワーク層でのリクエストの受け入れを停止し、Undertowはリクエストを受け入れるもののHTTP503で応答するようです。
Graceful Shutdown
では、具体的な実装例を見ていきましょう。
Spring WebFlux/NettyでのGraceful Shutdown Spring WebFluxは、NettyなどのノンブロッキングI/Oベースのアプリケーションサーバ上で動作します。そして、ネットワークアクセスやDBアクセスなどのI/O処理の呼び出しに対して、1つのスレッドで複数のリクエストを処理でき少ないスレッドで実行できるため、メモリサイズやCPUの使用を減らすことが可能です。
Spring WebFluxでは、Reactorによるリアクティブプログラミングを採用しています。リアクティブプログラミングとは、データに着目したイベント駆動型のプログラミングの一種で、通知されるデータを受け取って処理を行うハンドラを実装することによって連続的なデータを処理する手法です。Reactorはリアクティブプログラミングを実現するためのライブラリの一つであり、ノンブロッキングで非同期なリアクティブプログラミングの仕組みを提供しています。
Spring WebFluxによるWebアプリケーションの作成 Spring WebFluxを使ったWebアプリケーションは「Spring Initializr」を使ってプロジェクトのひな型を作成できます。その際、[ADD DEPENDENCIES]で[Spring Reactive Web]と[Spring Boot Actuator]を追加します。
ダウンロードしたZIPのpom.xmlを確認すると以下のdependencyが登録されています。
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-webflux&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.</description></item><item><title>Spring WebFlux + WebClientでリトライ処理を実装する</title><link>https://asashiho.github.io/spring-webflux-retry/spring-webflux-retry/</link><pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/spring-webflux-retry/spring-webflux-retry/</guid><description>最近業務でお手伝いさせていただく案件のなかでも、Java on Kubernetesなパターンが増えてきています。Kubernetesに限らずクラウドのマネージドPaaSで動かすにはアプリケーション側でもクラウドネイティブを考慮した実装が必要になり、特に耐障害性や回復性などを意識した実装が大事になってきます。
クラウドネイティブなアプリケーション方式設計されたシステムでは、大きな問題もなく開発者からみると「クラウド超便利」「マネージド最高か！」となりますが、ごくまれに「オンプレで動いていたそのままのアプリをクラウドに持ってきた」or「オンプレを前提とした開発手法で実装されたレガシーアプリをクラウドネイティブなPaaSサービスで動かしてみた」というケースで問題が発生することがあります。
クラウドネイティブな環境でアプリケーションを動かすときに、開発者の観点でどういうところに注意すればよいかは、来月中にきちんとまとめる予定ですが
通信先のサービス(DB/外部API)は常に動いていると思うべからず 死に際を美しく(Graceful Shutdown) 秘匿情報・環境依存値の取り扱い ステートレスにすべし あたりが大事なポイントになってくるかなと思っています。
このエントリでは、１つ目の
通信先のサービス(DB/外部API)は常に動いていると思うべからず
対策の一つであるアプリケーションのリトライ処理の実装について、Spring WebFluxのWebClientを使った実装の例を説明します。
そもそも、リトライ処理とは？ クラウドとは？を開発者のみなさんに一言で説明すると、以下のとおりです。
「クラウド事業者が運用している巨大インフラ(サーバ/ネットワーク/ストレージ)を利用できるサービス。利用するにはAPIをCallするだけ、利用料金は従量課金、SLAが規定値を下回ったら返金 」
たとえば99.99%のクラウドサービスを利用した場合、1か月のうち、4.3分は停止する可能性があります。それが月次の定期メンテナンスのタイミング/深夜のユーザ利用率の低い時間帯/優先度の低いシステムであれば大きな問題にはならないかもしれません。しかし相手は機械。空気を読むことはしません。金融システムにおける五十日の決済処理のような場合も十分にありえます。あとは分かるな？
SLA 停止時間(月) 99.9% 43分 99.95% 21分 99.99% 4.3分 99.999% 25.8秒 そもそも、クラウドでなくとも多数のコンポーネントから構成される大規模分散システムでは系内の障害をゼロにすることは不可能なため、障害をゼロにしよう！というアプローチではなく、回復性を高めよう！という観点でのアプリケーション処理方式の一つとしてリトライ処理を実装されてきたとおもいます。特にミッションクリティカル系システムでは、いにしえより必ず検討されてきた方式なので、令和の今あらためての新鮮味はないでしょう。
とはいえ、リトライ処理とは文字通り「リクエストを再試行する」ものなのですが、いつ、だれが、どこで、どうやってリトライを行えばいいのかはむずかしい問題です。
Exponential backoff and jitter ―― 指数関数的バックオフとジッター リクエストもまばらでの規模が小さい場合は、n秒ごとに再試行する、などの固定値で実装するケースもあるかとおもいますが、大規模ミッションクリティカルシステムの場合データセンター障害のような大規模なトラブル時を考慮する必要があります。大規模障害時は多くのサーバ/サービスが停止していると想定されます。そのため、多数のクライアントからのリクエストがエラーになり、結果的に多数のリトライが送信されることになります。
このとき、クライアント側で固定値でリトライ処理を実装すると、全クライアントがn秒ごとに同じタイミングでリトライ処理を実施することになります。API側からみると、n秒ごとに多数のリクエストに受けることになり、過剰な負荷がかかることになります。さらにリトライストームでカスケード障害を引き起こす可能性もあります。
そのため、リトライする間隔が大事になってきます。どのような間隔でリトライを行えば、系全体でみたときに効率的なのかを解決する手法として、「Exponential backoff and jitter」という有名なリトライのアルゴリズムがあります。
Exponential backoffは、処理が失敗した後のリトライする間隔を許容可能な範囲で徐々に減らしつつ継続するアルゴリズムです。具体的には、再試行する度に、1秒後、2秒後、4秒後と指数関数的に待ち時間を長くしていきます。これにより、全体のリトライ回数を抑え、リクエストが殺到するのを防ぎます。
一般的に分散システムでは複数のコンポーネントが協調して動作します。そのためリトライのタイミングをわざとずらして衝突を回避させるために、random関数を用いたjitterというゆらぎをいれます。
ちなみにAzureでは一時障害がなぜ発生するのか？どう対処すればよいか、アンチパターンは？などのガイダンスがまとまっておりますので、ぜひ。2016年のドキュメントですが、色褪せない内容になっています。 Transient fault handling</description></item><item><title>Azure AppServiceでOpenID Connect プロバイダーを使用してログインする</title><link>https://asashiho.github.io/appservice-oidc/appservice-oidc/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/appservice-oidc/appservice-oidc/</guid><description>※ 本エントリーは2021/09時点の内容になります。現在プレビュー機能のため内容が変更になる場合があります。
AzureのAppServiceは、認証プロバイダとしてAzure AD/Facebook/Google/Twitterに加えて、OpenID Connectを選ぶことができます。 このエントリーは、OpenID Connect の仕様に準拠したカスタム認証プロバイダーを使用してAzure App Servicesを構成する方法を説明します。
KeyCloakでOpenID Connectプロバイダーを構築する まずはじめに機能検証を行うために、OpenID Connectプロバイダーを構築します。既存のIDプロバイダーがある場合はそちらを使ってください。
KeyCloakの検証環境構築 ここでは、Azure Container InstanceとKeyCloakを使って簡易的なIDプロバイダを作成します。
まず、Azureの東日本リージョンにリソースグループを作成します
az login RG_NAME=keycloak ACI_NAME=keycloak az group create -n $RG_NAME -l japaneast Azure Container Instanceは(ACI)、Azureが提供するコンテナ実行環境サービスです。 次のコマンドを実行してKeyCloakをAzure Container Instanceを起動します。 ここで、'&amp;ndash;environment-variablesオプション&amp;rsquo; でKeyCloakの管理者ユーザ名を&amp;rsquo;KEYCLOAK_USER&amp;rsquo;、パスワードを&amp;rsquo;KEYCLOAK_PASSWORD'に設定します。
az container create \ -g $RG_NAME \ -n $ACI_NAME \ --image jboss/keycloak \ --dns-name-label aci-keycloak \ --ports 8080 \ --environment-variables &amp;#39;KEYCLOAK_USER&amp;#39;=&amp;#39;admin&amp;#39; &amp;#39;KEYCLOAK_PASSWORD&amp;#39;=&amp;#39;password&amp;#39; 次のコマンドを実行すると、ACIの完全修飾ドメイン名(FQDN)とそのプロビジョニング状態が表示されます。
az container show \ -g $RG_NAME \ -n $ACI_NAME \ --query &amp;#34;{FQDN:ipAddress.</description></item><item><title>KubernetesクラスターでのWebアプリケーションゲートウェイ(Ingress)の構成について整理する</title><link>https://asashiho.github.io/aks-ingress/kubernetes-ingress/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/aks-ingress/kubernetes-ingress/</guid><description>前回のブログでは、Kubernetesクラスターに対するL4アクセス制御をAzure Kubernetes Serviceを例にして整理しました。
しかしながら、多くのケースではWebアプリケーションへの急激なトラフィックの増加やトランザクションの集中にも対応できるようにするだけでなく、バックエンドサーバー群に効率よくルーティングする機能やSSL終端、またDDoS攻撃からの防御やSQLインジェクションなどから保護するWeb Application FirewallなどL7でのアクセス制御が必要です。さらに流量制御や証明書管理などなど、さまざまな非機能要件があります。
で、基盤方式設計フェーズでは、これらの処理をだれがどこでどう行うか？の処理方式を検討しておく必要があるわけですが。。。
レジェンドな企業の多くの場合は、アプリケーション開発部門とインフラ構築部門、運用保守部門が組織として分かれているケースが多く、Webアプリケーションのゲートウェイ機能の方式設計は、インフラ構築部門が行い、共通基盤化してアプリ開発チームに引き渡し、運用フェーズになると運用保守部門にてメンテナンスすることがおおいでしょう。 大規模なシステムの場合、組織での責任分界点も踏まえたうえで、基盤方式設計を行うというのは極めて大事です。
システム内でKubernetesを利用する場合、L7ロードバランサの方式設計時に考慮しておくべきことがいくつかありますので、簡単なメモ書きとしてまとめます。
Kubernetes でのWebアプリケーションゲートウェイの構成パターン Kubernetes では、L7ロードバランシングを提供するとき、「Ingress」というリソースを使います。これは、HTTP やHTTPS の外部アクセスを制御するオブジェクトで、バーチャルホストやURLルーティング、ロードバランシング、SSL 終端などの機能を提供するものです。
Kubernetes のIngressには大きく分けて次の2つがあります。
クラスタ内でゲートウェイ処理を行うパターン これは、Kubernetes クラスター内でソフトウエアとしてWebアプリケーションゲートウェイの機能を動かすパターンです。Kubernetesクラスターからみるとコンテナアプリケーション(Pod)として動作し、これがミドルウエアの機能を果たします。
代表的なものがNginx Ingress Controller で、そのほかにもContour やContainous 社のTraefik、HAProxy などがあります。
クラスタ外でゲートウェイ処理を行うパターン ゲートウェイで処理したいもののなかには、SSL 終端のようにコンピューティングリソースを多く使うものもあったりします。 そのため、ゲートウェイ処理に特化したのものをクラスタ外部で動かし、クラスタとは疎結合にしておきたい！というニーズもあります。 普段オンプレをメインに担当されている方は、直感的に理解できるかと思います。
たとえばAzureの場合「Azure Application Gateway」という専用のサービスがあり、バックエンドがKuberbetes に限らずAzure 上の仮想マシンやオンプレなどのバックエンドもサポートできるものが提供されています。 またGCP の場合だと「Cloud Load Balancing」という最強サービスがあります。
これらを使ってバックエンドにKuberbetes クラスターをおいてあげれば、よさそうです。
Web アプリケーションゲートウェイの設定はどう運用するのか？ URLルーティングなどはアプリケーション開発者の関心ごとである一方、特にマルチクラウドでシステムを運用している場合、各クラウドごとに実装の異なるL7 ロードバランサーの操作手順をいちいち勉強するのはあまりテンションがあがるものではないでしょう。
しかもそれがものすごい勢いでアップデートしていくわけなので、だれかに任せたい気分になるとおもいます。
KubernetesでWeb アプリケーションゲートウェイの設定変更したり監視したりなどの運用保守を考えるとき、次の2つのパターンが考えられます。
運用管理者が行う方法 オンプレや仮想マシンベースのアーキテクチャの場合、URL ルーティングのポリシーの管理などはシステム変更があるたびにアプリケーション開発者からインフラ運用部門に変更依頼や承認などを経て、人間の手を介して本番環境へ反映するケースが多いと思います。
しかしながら、人間には信頼性がないことが広く知られています。
万全な冗長構成をとり基盤テストを繰り返して99.99999…を追求し、広域災害にも配慮した方式設計にすることでシステムの可用性もコストも高まり、クラウドベンダーは喜びます。しかしながら、SLAのない人間による手作業による温かみのあるURL ルーティングの設定変更がsingle point of failure になる可能性もありますので、運用方式設計にきちんと盛り込んでおきましょう！
が、すでに既存のシステムでなんらかのWeb アプリケーションゲートウェイを導入済みで、それらの運用スタイルが確立していて特に問題がないようであれば、使い慣れたものをそのまま利用するのもよいでしょう。
Azure の場合だと、すでにApplication Gateway を稼働中であればバックエンドサーバー群にAKS を配置してロードバランシングやルーティングなどを配置すればよいでしょう。なによりシステム構成がシンプルで直感的です。 アプリケーション開発者と運用保守エンジニアの心の距離が近い、または同一人物である、などの組織であれば、フレキシブルに対応できると思います。</description></item><item><title>Kubernetesクラスターに対するL4ネットワークアクセス制御を整理する</title><link>https://asashiho.github.io/aks-l4/kubernetes-l4/</link><pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/aks-l4/kubernetes-l4/</guid><description>お仕事でプリセールスをしているため、お客様やSIパートナー様といっしょに「高い安全性が求められるシステム」について議論することが多くあります。
いわゆるクラウドネイティブアプリケーションの場合、クラウド時代の脅威に対応するためには「 内部を（ユーザーも含めて）信頼しない 」という考え方を前提に考えることが適していたりします。
境界ベースのセキュリティからゼロトラスト セキュリティへの移行
従来のセキュリティ モデルでは、組織のアプリケーションはプライベート データセンターの外部ファイアウォールに依存して、受信トラフィックを保護できません。クラウド ネイティブ環境でも、BeyondCorp モデルと同様にネットワーク境界を引き続き保護する必要がありますが、境界ベースのセキュリティ モデルでは十分に保護できません。新しいセキュリティ上の問題が発生するわけではありませんが、ファイアウォールで企業ネットワークを完全に保護できなければ、本番環境ネットワークも完全に保護できません。ゼロトラスト セキュリティ モデルでは、内部トラフィックが暗黙に信頼されず、認証や暗号化などのセキュリティ制御が必要になります。同時に、マイクロサービスへの移行は、従来のセキュリティ モデルを再検討する機会となります。単一のネットワーク境界（1 つのファイアウォールなど）への依存を排除すると、ネットワークをサービスごとに分割できるようになります。このアイデアをさらに進めて、サービス間に固有の相互信頼関係をなくして、マイクロサービス レベルのセグメンテーションを実装することもできます。マイクロサービスでは、トラフィックの信頼度がさまざまであるため、内部トラフィックと外部トラフィックを比較するだけでは済みません。
引用:BeyondProd: クラウド ネイティブ セキュリティに対する新しいアプローチより
とはいうものの、、、オンプレミス環境などでシステムを運用しているケースでは境界型モデルが馴染みが深く、まず最初に方式設計の検討テーマに上がります。
このブログでは、Kubernetesクラスターへのネットワークアクセスについて私自身が検証していて気づいたことをホワイトボードに書く感覚で、だらだらと書きとめます。
※ なお「Kuberntesのセキュリティ対策はネットワークアクセス制御さえすれば安全といえよう！」のようなシンプルなものではなく、認証認可・イメージの脆弱性検査・特権エスカレーションの阻止などなどなどなどなど、多岐にわたります。Kuberntesに対する脅威はこちらの「Attack matrix for Kubernetes」にまとまっておりますので、ぜひ。
Kubernetesクラスターへの2つのアクセス経路 Kubernetesクラスターを運用環境で稼働させるとき、安全なアクセス経路を検討するには、次の2つを考える必要があります。
なお、分かりやすさ重視のため「ほげ銀行」という妄想上の企業の社内システムを例に説明しています。
1. Kubernetesクラスター上で動かすアプリに対するアクセス 主にシステム利用者がKubernetesクラスター上で動くWebアプリケーションにアクセスするときのアクセス経路です。たとえば、ほげ銀行では市場国際部門・与信管理部門・証券投資部門のユーザがそれぞれの業務を行うためのアプリケーションにアクセスします。
2. Kubernetesクラスター管理のためのコントロープレーンに対するアクセス Kubernetesクラスターへのアプリのデプロイやクラスターの運用などでKubernetesのAPI Serverに対してアクセスするための経路です。クラスターに対して強い権限が必要になるため、限られたアクセス経路からのみ通信できる必要があります。
今回のブログは、1.のケースのアクセス経路についてのみ整理します。
(この話をごちゃごちゃ混乱させずに文章で説明する能力がないので2.のプライベートクラスターのケースについては、別ブログにて改めて)
境界型モデルによるネットワークアクセスコントロール 従来のオンプレミス型のシステムの場合、業務システムへのアクセスは、すべてのトラフィックをFWを介してコントロールする場合が多いと思います。
基本的に社内システムはプライベートな閉塞区間で運用し、社外への通信が必要な場合はバリアセグメントを設け、そこを経由することでアクセスコントロール・監査・Lockdownを1か所で行う、という考え方です。
たとえば、Azureの場合、Azure Kubernetes Service(AKS)のアクセス制御するときは以下のようなアーキテクチャになります。
このクラスタ外に配置したAzure Firewallの主な目的は、組織がAKSクラスターに対して細かくIngressおよびEgress Traffic規則を設定できるようにすることです。
たとえばこの構成例では、Ingress Trafficはファイアウォール フィルターの経由が強制されています。 またEgress Trafficはユーザー定義ルートを使用して、NodeからAzure FirewallのInternal IPを介して、パブリックインターネットまたはその他のAzureサービスへのアクセスは、FWのPublic IPで行われます。
またAKSの場合、クラスターが機能するには、必須のEgress Endpointで定義されているすべてのEndpointをアプリケーション ファイアウォール規則で有効にする必要があります。 これらのエンドポイントが有効ではない場合、クラスターは正しく動かないので注意してください。
az network firewall application-rule create -g $RG -f $FWNAME \ --collection-name &amp;#39;AKS_Global_Required&amp;#39; \ --action allow \ --priority 100 \ -n &amp;#39;required&amp;#39; \ --source-addresses &amp;#39;*&amp;#39; \ --protocols &amp;#39;http=80&amp;#39; &amp;#39;https=443&amp;#39; \ --target-fqdns \ &amp;#39;aksrepos.</description></item></channel></rss>