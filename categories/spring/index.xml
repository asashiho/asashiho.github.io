<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spring on ASA Blog</title><link>https://asashiho.github.io/categories/spring/</link><description>Recent content in spring on ASA Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>Your Copyright</copyright><lastBuildDate>Fri, 24 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://asashiho.github.io/categories/spring/index.xml" rel="self" type="application/rss+xml"/><item><title>Spring WebFlux + WebClientでリトライ処理を実装する</title><link>https://asashiho.github.io/spring-webflux-retry/spring-webflux-retry/</link><pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/spring-webflux-retry/spring-webflux-retry/</guid><description>最近業務でお手伝いさせていただく案件のなかでも、Java on Kubernetesなパターンが増えてきています。Kubernetesに限らずクラウドのマネージドPaaSで動かすにはアプリケーション側でもクラウドネイティブを考慮した実装が必要になり、特に耐障害性や回復性などを意識した実装が大事になってきます。
しっかりアプリケーション方式設計されたシステムでは、大きな問題もなく開発者からみると「クラウド超便利」「マネージド最高か！」となりますが、ごくまれに「オンプレで雰囲気で動いていたそのままのアプリをクラウドに持ってきた」or「オンプレを前提とした開発手法で実装されたレガシーアプリをクラウドネイティブなPaaSサービスで動かしてみた」というケースで問題が発生することがあります。
クラウドネイティブな環境でアプリケーションを動かすときに、開発者の観点でどういうところに注意すればよいかは、来月中にきちんとまとめる予定ですが
通信先のサービス(DB/外部API)は常に動いていると思うべからず 死に際を美しく(Graceful Shutdown) 秘匿情報・環境依存値の取り扱い ステートレスにすべし あたりが大事なポイントになってくるかなと思っています。
このエントリでは、１つ目の
通信先のサービス(DB/外部API)は常に動いていると思うべからず
対策の一つであるアプリケーションのリトライ処理の実装について、Spring WebFluxのWebClientを使った実装の例を説明します。
そもそも、リトライ処理とは？ プリセールスなので、クラウドとは？わかりやすく説明して！といわれると「お客様のDXを実現するもので、ビジネス変化の速度をアクセラレートする(2階微分)ために不可欠なものです」みたいなことを言ってしまいがちですが、、、
アプリケーション開発者のみなさまにクラウドを一言で説明せよ、となると
「クラウド事業者が運用している巨大インフラ(サーバ/ネットワーク/ストレージ)を利用できるサービス。利用するにはAPIをCallするだけ、利用料金は従量課金、SLAが規定値を下回ったら返金 」
というごくシンプルなものです。
たとえば99.99%のクラウドサービスを利用した場合、1か月のうち、4.3分は停止する可能性があります。それが月次の定期メンテナンスのタイミング/深夜のユーザ利用率の低い時間帯/優先度の低いシステムであれば大きな問題にはならないかもしれません。しかし相手は機械。空気を読むことはしません。金融システムにおける五十日の決済処理のような場合も十分にありえます。あとは分かるな？
SLA 停止時間(月) 99.9% 43分 99.95% 21分 99.99% 4.3分 99.999% 25.8秒 そもそも、クラウドでなくとも多数のコンポーネントから構成される大規模分散システムでは系内の障害をゼロにすることは不可能なため、障害をゼロにしよう！というアプローチではなく、回復性を高めよう！という観点でのアプリケーション処理方式の一つとしてリトライ処理を実装されてきたとおもいます。特にミッションクリティカル系システムでは、いにしえより必ず検討されてきた方式なので、令和の今あらためての新鮮味はないでしょう。
とはいえ、リトライ処理とは文字通り「リクエストを再試行する」ものなのですが、いつ、だれが、どこで、どうやってリトライを行えばいいのかはむずかしい問題です。
Exponential backoff and jitter ―― 指数関数的バックオフとジッター リクエストもまばらでの規模が小さい場合は、n秒ごとに再試行する、などの固定値で実装するケースもあるかとおもいますが、大規模ミッションクリティカルシステムの場合データセンター障害のような大規模なトラブル時を考慮する必要があります。大規模障害時は多くのサーバ/サービスが停止していると想定されます。そのため、多数のクライアントからのリクエストがエラーになり、結果的に多数のリトライが送信されることになります。
このとき、クライアント側で固定値でリトライ処理を実装すると、全クライアントがn秒ごとに同じタイミングでリトライ処理を実施することになります。API側からみると、n秒ごとに多数のリクエストに受けることになり、過剰な負荷がかかることになります。さらにリトライストームでカスケード障害を引き起こす可能性もあります。
そのため、リトライする間隔が大事になってきます。どのような間隔でリトライを行えば、系全体でみたときに効率的なのかを解決する手法として、「Exponential backoff and jitter」という有名なリトライのアルゴリズムがあります。
Exponential backoffは、処理が失敗した後のリトライする間隔を許容可能な範囲で徐々に減らしつつ継続するアルゴリズムです。具体的には、再試行する度に、1秒後、2秒後、4秒後と指数関数的に待ち時間を長くしていきます。これにより、全体のリトライ回数を抑え、リクエストが殺到するのを防ぎます。
一般的に分散システムでは複数のコンポーネントが協調して動作します。そのためリトライのタイミングをわざとずらして衝突を回避させるために、random関数を用いたjitterというゆらぎをいれます。
ちなみにAzureでは一時障害がなぜ発生するのか？どう対処すればよいか、アンチパターンは？などのガイダンスがまとまっておりますので、ぜひ。2016年のドキュメントですが、色褪せない内容になっています。 Transient fault handling</description></item></channel></rss>