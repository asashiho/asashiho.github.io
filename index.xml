<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ASA Blog</title><link>https://asashiho.github.io/</link><description>Recent content on ASA Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>Your Copyright</copyright><lastBuildDate>Fri, 08 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://asashiho.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetesクラスターに対するL4ネットワークアクセス制御を整理する</title><link>https://asashiho.github.io/aks-l4/kubernetes-l4/</link><pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/aks-l4/kubernetes-l4/</guid><description>お仕事でプリセールスをしているため、お客様やSIパートナー様といっしょに「高い安全性が求められるシステム」について議論することが多くあります。
いわゆるクラウドネイティブアプリケーションの場合、クラウド時代の脅威に対応するためには「 内部を（ユーザーも含めて）信頼しない 」という考え方を前提に考えることが適していたりします。
境界ベースのセキュリティからゼロトラスト セキュリティへの移行
従来のセキュリティ モデルでは、組織のアプリケーションはプライベート データセンターの外部ファイアウォールに依存して、受信トラフィックを保護できません。クラウド ネイティブ環境でも、BeyondCorp モデルと同様にネットワーク境界を引き続き保護する必要がありますが、境界ベースのセキュリティ モデルでは十分に保護できません。新しいセキュリティ上の問題が発生するわけではありませんが、ファイアウォールで企業ネットワークを完全に保護できなければ、本番環境ネットワークも完全に保護できません。ゼロトラスト セキュリティ モデルでは、内部トラフィックが暗黙に信頼されず、認証や暗号化などのセキュリティ制御が必要になります。同時に、マイクロサービスへの移行は、従来のセキュリティ モデルを再検討する機会となります。単一のネットワーク境界（1 つのファイアウォールなど）への依存を排除すると、ネットワークをサービスごとに分割できるようになります。このアイデアをさらに進めて、サービス間に固有の相互信頼関係をなくして、マイクロサービス レベルのセグメンテーションを実装することもできます。マイクロサービスでは、トラフィックの信頼度がさまざまであるため、内部トラフィックと外部トラフィックを比較するだけでは済みません。
引用:BeyondProd: クラウド ネイティブ セキュリティに対する新しいアプローチより
とはいうものの、、、オンプレミス環境などでシステムを運用しているケースでは境界型モデルが馴染みが深く、まず最初に方式設計の検討テーマに上がります。
このブログでは、Kubernetesクラスターへのネットワークアクセスについて私自身が検証していて気づいたことをホワイトボードに書く感覚で、だらだらと書きとめます。
※ なお「Kuberntesのセキュリティ対策はネットワークアクセス制御さえすれば安全といえよう！」のようなシンプルなものではなく、認証認可・イメージの脆弱性検査・特権エスカレーションの阻止などなどなどなどなど、多岐にわたります。Kuberntesに対する脅威はこちらの「Attack matrix for Kubernetes」にまとまっておりますので、ぜひ。
Kubernetesクラスターへの2つのアクセス経路 Kubernetesクラスターを運用環境で稼働させるとき、安全なアクセス経路を検討するには、次の2つを考える必要があります。
なお、分かりやすさ重視のため「ほげ銀行」という妄想上の企業の社内システムを例に説明しています。
1. Kubernetesクラスター上で動かすアプリに対するアクセス 主にシステム利用者がKubernetesクラスター上で動くWebアプリケーションにアクセスするときのアクセス経路です。たとえば、ほげ銀行では市場国際部門・与信管理部門・証券投資部門のユーザがそれぞれの業務を行うためのアプリケーションにアクセスします。
2. Kubernetesクラスター管理のためのコントロープレーンに対するアクセス Kubernetesクラスターへのアプリのデプロイやクラスターの運用などでKubernetesのAPI Serverに対してアクセスするための経路です。クラスターに対して強い権限が必要になるため、限られたアクセス経路からのみ通信できる必要があります。
今回のブログは、1.のケースのアクセス経路についてのみ整理します。
(この話をごちゃごちゃ混乱させずに文章で説明する能力がないので2.のプライベートクラスターのケースについては、別ブログにて改めて)
境界型モデルによるネットワークアクセスコントロール 従来のオンプレミス型のシステムの場合、業務システムへのアクセスは、すべてのトラフィックをFWを介してコントロールする場合が多いと思います。
基本的に社内システムはプライベートな閉塞区間で運用し、社外への通信が必要な場合はバリアセグメントを設け、そこを経由することでアクセスコントロール・監査・Lockdownを1か所で行う、という考え方です。
たとえば、Azureの場合、Azure Kubernetes Service(AKS)のアクセス制御するときは以下のようなアーキテクチャになります。
このクラスタ外に配置したAzure Firewallの主な目的は、組織がAKSクラスターに対して細かくIngressおよびEgress Traffic規則を設定できるようにすることです。
たとえばこの構成例では、Ingress Trafficはファイアウォール フィルターの経由が強制されています。 またEgress Trafficはユーザー定義ルートを使用して、NodeからAzure FirewallのInternal IPを介して、パブリックインターネットまたはその他のAzureサービスへのアクセスは、FWのPublic IPで行われます。
またAKSの場合、クラスターが機能するには、必須のEgress Endpointで定義されているすべてのEndpointをアプリケーション ファイアウォール規則で有効にする必要があります。 これらのエンドポイントが有効ではない場合、クラスターは正しく動かないので注意してください。
az network firewall application-rule create -g $RG -f $FWNAME \ --collection-name &amp;#39;AKS_Global_Required&amp;#39; \ --action allow \ --priority 100 \ -n &amp;#39;required&amp;#39; \ --source-addresses &amp;#39;*&amp;#39; \ --protocols &amp;#39;http=80&amp;#39; &amp;#39;https=443&amp;#39; \ --target-fqdns \ &amp;#39;aksrepos.</description></item><item><title>オイラー法/ホイン法/ルンゲクッタ法をつかった常微分方程式の初期値問題</title><link>https://asashiho.github.io/ordinary-differential-equation/ode/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/ordinary-differential-equation/ode/</guid><description>ここでは、数値解析の応用分野である構造解析や熱流体解析などのシミュレーションの基本となる、常微分方程式の初期値問題に関する簡単な数値解析を紹介します。
一般に独立変数$x$に関する関数$y$の$m$階常微分方程式はつぎのとおりです。
$$ \dfrac{dy}{dx}=f(x,y,y&amp;rsquo;,y&amp;rsquo;',\cdot \cdot \cdot,y^{m-1}) $$
ここで区間$[a,b]$における$x=a$での$m$個の初期値を与え
$$ y(a)=y_0,; y&amp;rsquo;(a)=y&amp;rsquo;_0,; \cdot \cdot \cdot,y^{(m-1)}(a)= y^{m-1} $$
で関数$y$を求める問題を初期値問題と言います。
ここでもっとも簡単な1階常微分方程式の初期値問題を考えます。
$$ \dfrac{dy}{dx} =f(x,y) $$
ここで$x$の解析領域$[a,b]$を間隔 $h$ で $n=(b-a)/h$ 分割して、$x,x_1,x_2,\cdot \cdot \cdot,x_n$ からなる区分 $i(x_i \leqq x \leqq x_{i+1})$ で積分すると次の式になります。
$$ \int_{x_{i}}^{x_{i+1}} \dfrac {dx}{dx}dx =\int_{x_{i}}^{x_{i+1}} f(x,y) dx $$
これを代数方程式で表すと
$$ y_{i+1} = y_{i} + \int_{x_{i}}^{x_{i+1}} f(x,y) dx $$ となり、初期値$(x_0,y_0)$を与えて順次解くことで近似解が数列${y_i}$として求められます。
この差分式は$h,x_{i},y_{i}$の値を用いた増分関数$k(x_i,y_i,h)$と刻み幅$h$との積として表されます。
$$ y_{i+1} = y_{i} + hk(x_i,y_i,h) $$
このように、厳密解ではなく代数方程式による近似解を求めることを数値解析と呼びます。数値解析は、数学や物理学の分野で代数的な方法で解を得ることが不可能な解析学上の問題を数値を用いて近似的に解くときに用いられ、主に計算機をつかって近似解を求めます。
今回は例題として、以下の常微分方程式
$$ \begin{aligned} \dfrac{dy}{dx} = xy \end{aligned} $$</description></item><item><title>高可用性かつスケーラブルなKubernetesクラスターを運用するときに気を付けたいこと</title><link>https://asashiho.github.io/kube-deschesuler/kube-deschesuler/</link><pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate><guid>https://asashiho.github.io/kube-deschesuler/kube-deschesuler/</guid><description>お仕事でプリセールスをしているため、お客様やSIパートナー様といっしょに「ぜったいにサービスを止められないシステム」について議論することが多くあります。
一方、クラウドはオンプレに比べてスケーラブルな構成をとることが得意です。したがって、ユーザーの利用が時間的にばらつきがあるシステムやスパイクアクセスが発生するシステムなどの場合は、クラウドを提案する良いチャンスだったりもします。
このブログではそのようなニーズを満たすクラスターを運用するときに気を付けたいことや、私自身が検証していて気づいたことをホワイトボードに書く感覚で、だらだらと書きとめます。
なお、本内容はたまたまAzureが提供するKubernetesマネージドサービスである「Azure Kubernetes Service（以下AKS）」を使って検証しましたが、基本的な考え方はやGoogle CloudのGKEやAWSのEKSなど他クラウドでも同じだとおもいます。
クラスターを物理障害から守るアーキテクチャ Kubernetesはシステムのインフラストラクチャー部分の多くを抽象化しオートヒーリングや宣言ベースのデプロイメントが可能で、高可用性なシステム構築を実現することが可能なコンテナオーケストレータです。 が、その土台をささえる物理レイヤーはオンプレミスやクラウドにかかわらず、まじないや祈祷のたぐいで安定稼働しているわけではありません。
多くの場合ハードウエアはデータセンターに収容され、電源/ネットワーク/冷却装置/サーバー/ストレージなどを構成しているため、高可用性システムを構築するにはSPOF（単一障害点）がないようインフラを設計する必要があります。
またサーバー群にインストールするOSで脆弱性やバグなどが発生したときはバージョンアップなどを行う必要もあります。
AzureのAKSはマネージドサービスではありますが、中でKuberntesのNodeをAzureのVMを使って構成しています。AzureのVMの可用性を高める手法として「可用性ゾーン」というものがあります。
ざっくりってしまうと、たとえばAzureの東日本リージョンにはそれぞれ異なる電源/ネットワーク/冷却装置が別々の3つのデータセンター(ゾーン)があります。そのためアプリを3つのデータセンターで分散して動かしておけば、たとえどこかのデータセンター内で障害が発生しても、システムが継続できる可能性が高くなります。
またこの可用性ゾーンは「更新ドメイン」をもっており、メンテナンスや再起動でシステムが停止するリスクを減らすこともできます。
物理的に分散したKubernetesクラスターを作成する AKSには、可用性ゾーンを使用するKuberntesクラスターを作成することができます。この便利機能を使うと、次のようにKuberntesのNodeを分散して配置できます。
いくつかの制限事項がありますが、執筆時点(2020/03)では、次のリージョンの可用性ゾーンが使用できます。東日本リージョンでも利用できます。
米国中部 米国東部2 米国東部 フランス中部 東日本 北ヨーロッパ 東南アジア 英国南部 西ヨーロッパ 米国西部2 ゾーンの停止時、つまりデータセンター丸ごと障害時には、手動またはオートスケーラーを使用してNodeを再調整でき、たとえ1つのゾーンが使用不可になっても、アプリケーションは引き続き実行されます。
AKSクラスターの構築のしかたは公式ドキュメントのとおりですが、ポイントとしては以下のように、az aks createコマンドの引数に--zonesオプションを指定します。これで3つのゾーンにNodeが分散されます。
az aks create \ -g $RG_NAME \ -n $AKS_NAME \ --generate-ssh-keys \ --vm-set-type VirtualMachineScaleSets \ --load-balancer-sku standard \ --node-count 3 \ --zones 1 2 3 ※ 今回は検証のためなので、&amp;ndash;generate-ssh-keysオプションを指定していますが、本番環境ではきちんとSSH Keyを設定してください。
クラスターは約15分ほどで起動します。次のコマンドでNodeが分散されているのを確認してください。3台のNodeが東日本リーションのjapaneast-1/japaneast-2/japaneast-3にそれぞれ1台ずつ配置されているのが分かります。</description></item></channel></rss>